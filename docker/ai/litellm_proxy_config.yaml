# https://artificialanalysis.ai/models
# https://chat.lmsys.org/?leaderboard

router_settings:
  enable_pre_call_checks: true # 1. Enable pre-call checks

model_list:
  - model_name: best
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20240620
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: good
    litellm_params:
      model: groq/llama3-70b-8192
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: cheap
    litellm_params:
      model: groq/llama3-70b-8192
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: cheapest
    litellm_params:
      model: groq/llama3-8b-8192
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: fallbacks
    litellm_params:
      model: groq/llama3-8b-8192
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: embeddings
    litellm_params:
      model: text-embedding-3-small
      api_key: "os.environ/OPENAI_API_KEY"

  # Open AI
  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: "os.environ/OPENAI_API_KEY"

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: "os.environ/OPENAI_API_KEY"

  # ANTHROPIC
  - model_name: haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20240620
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # GROQ
  - model_name: mixtral
    litellm_params:
      model: groq/mixtral-8x7b-32768
      api_key: "os.environ/GROQ_API_KEY"
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: llama3-8
    litellm_params:
      model: groq/llama3-8b-8192
      api_key: "os.environ/GROQ_API_KEY"
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: llama3-70
    litellm_params:
      model: groq/llama3-70b-8192
      api_key: "os.environ/GROQ_API_KEY"
      input_cost_per_token: 0
      output_cost_per_token: 0

  # DeepInfra
  - model_name: llama3-70-deepinfra
    litellm_params:
      model: deepinfra/meta-llama/Meta-Llama-3-70B-Instruct
      api_key: "os.environ/DEEPINFRA_API_KEY"
      input_cost_per_token: 0.00000056
      output_cost_per_token: 0.00000077
      
  - model_name: qwen2
    litellm_params:
      model: deepinfra/Qwen/Qwen2-72B-Instruct
      api_key: "os.environ/DEEPINFRA_API_KEY"
      input_cost_per_token: 0.00000056
      output_cost_per_token: 0.00000077
      max_input_tokens: 120000
      max_output_tokens: 4000
      
  - model_name: nemotron
    litellm_params:
      model: deepinfra/nvidia/Nemotron-4-340B-Instruct
      api_key: "os.environ/DEEPINFRA_API_KEY"
      input_cost_per_token: 0.0000042
      output_cost_per_token: 0.0000042
      
  
  # Google
  - model_name: google-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: "os.environ/GEMINI_API_KEY"
  
  - model_name: gemini-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: "os.environ/GEMINI_API_KEY"

  - model_name: google-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: "os.environ/GEMINI_API_KEY"

  - model_name: gemini-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: "os.environ/GEMINI_API_KEY"
  
  # Vertex

  # https://console.cloud.google.com/vertex-ai
  - model_name: gemini-flash-vertex
    litellm_params:
      model: vertex_ai/gemini-1.5-flash
      vertex_project: my-llm-project-427402
      vertex_location: "us-central1"

  - model_name: gemini-pro-vertex
    litellm_params:
      model: vertex_ai/gemini-1.5-pro
      vertex_project: my-llm-project-427402
      vertex_location: "us-central1"


  # HUGGINGFACE
  - model_name: mistral7
    litellm_params:
      model: huggingface/mistralai/Mistral-7B-Instruct-v0.3
      api_key: "os.environ/HUGGINGFACE_API_KEY"

  - model_name: llama3-8-hf
    litellm_params:
      model: huggingface/meta-llama/Meta-Llama-3-8B-Instruct
      api_key: "os.environ/HUGGINGFACE_API_KEY"
  # - model_name: qwen2-hf # Too big to run
  #   litellm_params:
  #     model: huggingface/Qwen/Qwen2-72B-Instruct
  #     api_key: "os.environ/HUGGINGFACE_API_KEY"
  
  # PERPLEXITY
  # Pricing: https://docs.perplexity.ai/docs/pricing
  - model_name: pplx
    litellm_params:
      model: perplexity/llama-3-sonar-small-32k-online
      api_key: "os.environ/PERPLEXITYAI_API_KEY"
      input_cost_per_token: 0.0000002
      output_cost_per_token: 0.0000002
      input_cost_per_request: 0.005

  - model_name: pplx-large
    litellm_params:
      model: perplexity/llama-3-sonar-large-32k-online
      api_key: "os.environ/PERPLEXITYAI_API_KEY"
      input_cost_per_token: 0.000001
      output_cost_per_token: 0.000001
      input_cost_per_request: 0.005

  # COHERE
  - model_name: command-r
    litellm_params:
      model: command-r
      api_key: "os.environ/COHERE_API_KEY"

  - model_name: command-r-plus
    litellm_params:
      model: command-r-plus
      api_key: "os.environ/COHERE_API_KEY"
  
litellm_settings:
  
  num_retries: 3 # retry call 3 times on each model_name (e.g. translate).
  allowed_fails: 3 # cooldown model if it fails > 1 call in a minute. 
  request_timeout: 100 # raise Timeout error if call takes longer than 10s. Sets litellm.request_timeout
  fallbacks: [{"paraphrase": ["default"]}] # fallback to default model if paraphrase model fails num_retries
  telemetry: False
  drop_params: True # Ignore parameter that the model doesn't understand
  success_callback: ["langfuse","datadog"]
  failure_callback: ["langfuse"]
  context_window_fallbacks:
    - best: 
      - gemini-pro
    - good: 
      - qwen2
      - sonnet
      - gemini-pro
    - cheap: 
      - mixtral
      - gemini-flash
    - cheapest: 
      - mixtral
      - haiku
      - gemini-flash
    - fallbacks: 
      - gemini-flash
  fallbacks:
    - gemini-pro:
      - gemini-pro-vertex
    - gemini-flash:
      - gemini-flash-vertex
    - cheap: 
      - gemini-flash
    - good: 
      - qwen2
      - sonnet
      - gemini-pro
    - llama3-70:
      - llama3-70-deepinfra


  # For Production:
  set_verbose: False
  json_logs: True
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
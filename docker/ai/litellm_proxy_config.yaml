# https://artificialanalysis.ai/models
# https://chat.lmsys.org/?leaderboard

router_settings:
  enable_pre_call_checks: true # 1. Enable pre-call checks

model_list:
  - model_name: default
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20240620
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: best
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20240620
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: good
    litellm_params:
      model: groq/llama3-70b-8192
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: cheap
    litellm_params:
      model: groq/llama3-70b-8192
      api_key: "os.environ/GROQ_API_KEY"

  # - model_name: cheapest
  #   litellm_params:
  #     model: groq/llama3-70b-8192
  #     api_key: "os.environ/GROQ_API_KEY"

  - model_name: fallbacks
    litellm_params:
      model: groq/llama3-8b-8192
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: embeddings
    litellm_params:
      model: text-embedding-3-small
      api_key: "os.environ/OPENAI_API_KEY"

  # Open AI
  - model_name: gpt4
    litellm_params:
      model: gpt-4o
      api_key: "os.environ/OPENAI_API_KEY"
  - model_name: gpt35
    litellm_params:
      model: gpt-3.5-turbo
      api_key: "os.environ/OPENAI_API_KEY"

  # ANTHROPIC
  - model_name: haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: "os.environ/ANTHROPIC_API_KEY"
  - model_name: sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20240620
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # GROQ
  - model_name: mixtral
    litellm_params:
      model: groq/mixtral-8x7b-32768
      api_key: "os.environ/GROQ_API_KEY"
  - model_name: llama3-8
    litellm_params:
      model: groq/llama3-8b-8192
      api_key: "os.environ/GROQ_API_KEY"
  - model_name: llama3-70
    litellm_params:
      model: groq/llama3-70b-8192
      api_key: "os.environ/GROQ_API_KEY"

  # Google
  - model_name: google-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: "os.environ/GEMINI_API_KEY"
  
  - model_name: gemini-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: "os.environ/GEMINI_API_KEY"

  # https://console.cloud.google.com/vertex-ai
  - model_name: gemini-flash-vertex
    litellm_params:
      model: vertex_ai_beta/gemini-1.5-flash
      vertex_project: my-llm-project-427402
      vertex_location: "us-central1"

  - model_name: google-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: "os.environ/GEMINI_API_KEY"

  - model_name: gemini-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: "os.environ/GEMINI_API_KEY"

  - model_name: gemini-pro-vertex
    litellm_params:
      model: vertex_ai_beta/gemini-1.5-pro-001
      vertex_project: my-llm-project-427402
      vertex_location: "us-central1"
  
  # HUGGINGFACE
  - model_name: mistral7
    litellm_params:
      model: huggingface/mistralai/Mistral-7B-Instruct-v0.3
      api_key: "os.environ/HUGGINGFACE_API_KEY"
  
  # PERPLEXITY
  - model_name: pplx
    litellm_params:
      model: perplexity/llama-3-sonar-small-32k-online
      api_key: "os.environ/PERPLEXITYAI_API_KEY"
  - model_name: pplx-large
    litellm_params:
      model: perplexity/llama-3-sonar-large-32k-online
      api_key: "os.environ/PERPLEXITYAI_API_KEY"
      
  
litellm_settings:
  
  num_retries: 3 # retry call 3 times on each model_name (e.g. translate).
  request_timeout: 100 # raise Timeout error if call takes longer than 10s. Sets litellm.request_timeout
  fallbacks: [{"paraphrase": ["default"]}] # fallback to default model if paraphrase model fails num_retries
  telemetry: False
  drop_params: True # Ignore parameter that the model doesn't understand
  success_callback: ["langfuse","datadog"]
  context_window_fallbacks:
    - best: 
      - gemini-pro
    - good: 
      - sonnet
      - gemini-pro
    - cheap: 
      - gemini-flash
    - fallbacks: 
      - gemini-flash
  fallbacks:
    - gemini-pro:
      - gemini-pro-vertex
    - gemini-flash:
      - gemini-flash-vertex
    - cheap: 
      - gemini-flash

  allowed_fails: 3 # cooldown model if it fails > 1 call in a minute. 

  # For Production:
  set_verbose: False
  json_logs: True
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"